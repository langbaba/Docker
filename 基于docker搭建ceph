

192.168.4.1 br-sh-sip-1 mon、osd、mgr、rgw
192.168.4.9 br-sh-docker-1 mon、osd、mgr
192.168.4.10 br-sh-pm-1 mon、osd、mgr


一、安装ceph集群
所有服务器器上/etc/hosts⽂文件⾥里里增加（必须是实际的主机名）：
192.168.4.1 br-sh-sip-1
192.168.4.9 br-sh-docker-1
192.168.4.10 br-sh-pm-1
在所有服务器器上执⾏行行：
mkdir -p /home/ceph/etc/ceph
mkdir -p /home/ceph/var/lib/ceph
注意：
所有服务器器允许root⽤用户ssh登录。
所有服务器器sshd监听在22端⼝口。
这个版本的ceph镜像运⾏行行docker exec ceph-mon ceph dashboard create-self-signed-cert 命令提示出错。
192.168.4.1上的操作：
#建⽴立ssh互信
ssh-keygen
ssh-copy-id br-sh-docker-1
ssh-copy-id br-sh-pm-1
#启动mon节点：
docker run -d --net=host --name ceph-mon -v /home/ceph/etc/ceph/:/etc/ceph -v /home/ceph/var/lib/ceph/:/var/lib/ceph -e CEPH_DAEMON=MON -e
MON_IP=192.168.4.1 -e CEPH_PUBLIC_NETWORK=192.168.4.0/24 ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mon
⾃自动⽣生成 /home/ceph/etc/ceph/ceph.conf
[global]
fsid = e821ab20-e4d7-4511-acc0-cc76a3ef80f4
mon initial members = br-sh-sip-1
mon host = 192.168.4.1
public network = 192.168.4.0/24
cluster network = 192.168.4.0/24
osd journal size = 100
log file = /dev/null
⾃自动⽣生成 /home/ceph/etc/ceph/ceph.mon.keyring
[mon.]
key = AQCtPVldVY+mBxAAhZ2aBOUq9dYF4TdSj4XGLQ==
caps mon = "allow *"
[client.admin]
key = AQCtPVlddkJaBBAA5hg10Kcg6BgpFk02k0ACCg==
auid = 0
caps mds = "allow"
caps mgr = "allow *"
caps mon = "allow *"
caps osd = "allow *"
[client.bootstrap-mds]
key = AQCtPVld20xWDhAAAnkE2s8wueIg2t8gjzSzXw==
caps mon = "allow profile bootstrap-mds"
[client.bootstrap-osd]
key = AQCtPVldU8T0ChAAdzqbeYN+Rx4uuFiflsIwZQ==
caps mon = "allow profile bootstrap-osd"
[client.bootstrap-rbd]
key = AQCtPVldtDB8FBAAFqM7EETIzK2hwCZLNX5Asw==
caps mon = "allow profile bootstrap-rbd"
[client.bootstrap-rgw]
key = AQCtPVldfaZ5ERAAdH0AuYYx+igikOcQdgpAIg==
caps mon = "allow profile bootstrap-rgw"
⾃自动⽣生成 /home/ceph/etc/ceph/ceph.client.admin.keyring
[client.admin]
key = AQCtPVlddkJaBBAA5hg10Kcg6BgpFk02k0ACCg==
auid = 0
caps mds = "allow"
caps mgr = "allow *"
caps mon = "allow *"
caps osd = "allow *"
#拷⻉贝配置⽂文件和系统⽂文件到其他两个节点（以root身份运⾏行行）
scp -r /home/ceph/etc/ceph/ br-sh-docker-1:/home/ceph/etc/
scp -r /home/ceph/etc/ceph/ br-sh-pm-1:/home/ceph/etc/
scp -r /home/ceph/var/lib/ceph br-sh-docker-1:/home/ceph/var/lib/
scp -r /home/ceph/var/lib/ceph br-sh-pm-1:/home/ceph/var/lib/
192.168.4.9上的操作：
#启动mon节点：
docker run -d --net=host --name ceph-mon -v /home/ceph/etc/ceph/:/etc/ceph -v /home/ceph/var/lib/ceph/:/var/lib/ceph -e CEPH_DAEMON=MON -e
MON_IP=192.168.4.9 -e CEPH_PUBLIC_NETWORK=192.168.4.0/24 ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mon
192.168.4.10上的操作：
#启动mon节点：
docker run -d --net=host --name ceph-mon -v /home/ceph/etc/ceph/:/etc/ceph -v /home/ceph/var/lib/ceph/:/var/lib/ceph -e CEPH_DAEMON=MON -e
MON_IP=192.168.4.10 -e CEPH_PUBLIC_NETWORK=192.168.4.0/24 ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mon
192.168.4.1上的操作：
#启动osd节点：
docker run -d --net=host --privileged=true --name ceph-osd -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e
CEPH_DAEMON=OSD_DIRECTORY -v /dev/:/dev/ -e OSD_FORCE_ZAP=1 -e OSD_TYPE=directory ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-osd
192.168.4.9上的操作：
#启动osd节点：
docker run -d --net=host --privileged=true --name ceph-osd -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e
CEPH_DAEMON=OSD_DIRECTORY -v /dev/:/dev/ -e OSD_FORCE_ZAP=1 -e OSD_TYPE=directory ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-osd
192.168.4.10上的操作：
#启动osd节点：
docker run -d --net=host --privileged=true --name ceph-osd -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e
CEPH_DAEMON=OSD_DIRECTORY -v /dev/:/dev/ -e OSD_FORCE_ZAP=1 -e OSD_TYPE=directory ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-osd
192.168.4.1上看，有5个到192.168.4.1:6789的TCP连接：
ESTAB 0 0 192.168.4.1:6789 192.168.4.10:37100
ESTAB 0 0 192.168.4.1:6789 192.168.4.9:43514
ESTAB 0 0 192.168.4.1:6789 192.168.4.1:59644
ESTAB 0 0 192.168.4.1:6789 192.168.4.9:43486
ESTAB 0 0 192.168.4.1:6789 192.168.4.10:37388
ESTAB 0 0 192.168.4.1:59644 192.168.4.1:6789
192.168.4.1上的操作：
#启动rgw节点
docker run -d --net=host --name ceph-rgw -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e CEPH_DAEMON=RGW
ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-rgw
#启动mgr节点
docker run -d --net=host --name ceph-mgr -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e CEPH_DAEMON=MGR
ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mgr
#启动mds节点（mds节点只在主节点启动）
docker run -d --net=host --name ceph-mds -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e CEPH_DAEMON=MDS
ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mds
docker exec ceph-mon ceph mds stat
cephfs-1/1/1 up {0=br-sh-sip-1=up:active}
docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS
PORTS NAMES
d3f2858a1b95 ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 "/entrypoint.sh ce..." About a minute ago Up About a minute
ceph-mds
f4c43549bcbd ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 "/entrypoint.sh ce..." About an hour ago Up About an hour
ceph-mgr
fca33ac4a5bb ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 "/entrypoint.sh ce..." About an hour ago Up About an hour
ceph-mon
c579cbbe5130 ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 "/entrypoint.sh ce..." 2 hours ago Up 2 hours
ceph-osd
0b90b0807b1b crapworks/ceph-dash:latest "/cephdash/contrib..." 6 days ago Up 6 days
0.0.0.0:35000->5000/tcp lucid_nobel
aae7cad10b4c ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 "/entrypoint.sh ce..." 6 days ago Up 6 days
ceph-rgw
docker exec ceph-mon ceph -s
cluster:
id: e821ab20-e4d7-4511-acc0-cc76a3ef80f4
health: HEALTH_OK
services:
mon: 3 daemons, quorum br-sh-sip-1,br-sh-docker-1,br-sh-pm-1
mgr: br-sh-sip-1(active), standbys: br-sh-docker-1, br-sh-pm-1
osd: 3 osds: 3 up, 3 in
rgw: 1 daemon active
data:
pools: 4 pools, 32 pgs
objects: 187 objects, 1113 bytes
usage: 522 GB used, 8625 GB / 9148 GB avail
pgs: 32 active+clean
docker exec ceph-mon ceph mgr dump
{
"epoch": 196,
"active_gid": 4128,
"active_name": "br-sh-sip-1",
"active_addr": "192.168.4.1:6804/121",
"available": true,
"standbys": [
{
"gid": 4134,
"name": "br-sh-docker-1",
"available_modules": [
"balancer",
"dashboard",
"influx",
"localpool",
"prometheus",
"restful",
"selftest",
"status",
"zabbix"
]
},
{
"gid": 4140,
"name": "br-sh-pm-1",
"available_modules": [
"balancer",
"dashboard",
"influx",
"localpool",
"prometheus",
"restful",
"selftest",
"status",
"zabbix"
]
}
],
"modules": [
"balancer",
"restful",
"status"
],
"available_modules": [
"balancer",
"dashboard",
"influx",
"localpool",
"prometheus",
"restful",
"selftest",
"status",
"zabbix"
],
"services": {}
}
docker exec ceph-mgr ceph mgr module enable dashboard
192.168.4.9上的操作：
#启动mgr节点
docker run -d --net=host --name ceph-mgr -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e CEPH_DAEMON=MGR
ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mgr
192.168.4.10上的操作：
#启动mgr节点
docker run -d --net=host --name ceph-mgr -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e CEPH_DAEMON=MGR
ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mgr
192.168.4.1上的操作：
检查osd tree：
docker exec ceph-mon ceph osd tree
ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF
-1 8.93446 root default
-5 1.75809 host br-sh-docker-1
1 hdd 1.75809 osd.1 up 1.00000 1.00000
-7 5.42299 host br-sh-pm-1
2 hdd 5.42299 osd.2 up 1.00000 1.00000
-3 1.75339 host br-sh-sip-1
0 hdd 1.75339 osd.0 up 1.00000 1.00000
docker exec ceph-mon ceph osd crush add osd.0 0.15 host=br-sh-sip-1
set item id 0 name 'osd.0' weight 0.15 at location {host=br-sh-sip-1} to crush map
docker exec ceph-mon ceph osd crush add osd.1 0.15 host=br-sh-sip-1
add item id 1 name 'osd.1' weight 0.15 at location {host=br-sh-sip-1} to crush map
docker exec ceph-mon ceph osd crush add osd.2 0.15 host=br-sh-sip-1
add item id 2 name 'osd.2' weight 0.15 at location {host=br-sh-sip-1} to crush map
docker exec ceph-mon ceph osd tree
ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF
-1 5.87297 root default
-5 0 host br-sh-docker-1
-7 5.42299 host br-sh-pm-1
2 hdd 5.42299 osd.2 up 1.00000 1.00000
-3 0.14999 host br-sh-sip-1
2 hdd 0.14999 osd.2 up 1.00000 1.00000
0 hdd 0.14999 osd.0 up 1.00000 1.00000
1 hdd 0.14999 osd.1 up 1.00000 1.00000
docker exec ceph-mon ceph osd crush move osd.0 root=default
moved item id 0 name 'osd.0' to location {root=default} in crush map
docker exec ceph-mon ceph osd crush move osd.1 root=default
moved item id 1 name 'osd.1' to location {root=default} in crush map
docker exec ceph-mon ceph osd crush move osd.2 root=default
moved item id 2 name 'osd.2' to location {root=default} in crush map
docker exec ceph-mon ceph osd tree
ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF
-1 0.44998 root default
-5 0 host br-sh-docker-1
-7 0 host br-sh-pm-1
-3 0 host br-sh-sip-1
0 hdd 0.14999 osd.0 up 1.00000 1.00000
1 hdd 0.14999 osd.1 up 1.00000 1.00000
2 hdd 0.14999 osd.2 up 1.00000 1.00000
⼆二、存储池相关操作
#列列出存储池
docker exec ceph-mon ceph osd lspools
1 .rgw.root,2 default.rgw.control,3 default.rgw.meta,4 default.rgw.log,
#创建testpool存储池
docker exec ceph-mon ceph osd pool create testpool 100
pool 'testpool' created
docker exec ceph-mon ceph osd pool create testpool-data 100
pool 'testpool-data' created
#查看存储池
docker exec ceph-mon ceph osd lspools
1 .rgw.root,2 default.rgw.control,3 default.rgw.meta,4 default.rgw.log,5 testpool,6 testpool-data,
#获取testpool存储的副本数
docker exec ceph-mon ceph osd pool get testpool size
size: 3
#查看testpool存储的pg_num和pgp_num
docker exec ceph-mon ceph osd pool get testpool pg_num
pg_num: 100
docker exec ceph-mon ceph osd pool get testpool pgp_num
pgp_num: 100
#公式：One pool total PGs = (Total_number_of_OSD * 100) / max_replication_count
#根据以上公式计算每个pool的pg数 = (3*100)/3 = 100，取最近的2的N次⽅方值，即128
#设置存储池的pg_num和pgp_num
docker exec ceph-mon ceph osd pool set testpool pg_num 128
docker exec ceph-mon ceph osd pool set testpool pgp_num 128
docker exec ceph-mon ceph osd pool set testpool-data pg_num 128
set pool 6 pg_num to 128
docker exec ceph-mon ceph osd pool set testpool-data pgp_num 128
set pool 6 pgp_num to 128
#查看存储池的pg_num和pgp_num
docker exec ceph-mon ceph osd pool get testpool pg_num
pg_num: 128
docker exec ceph-mon ceph osd pool get testpool pgp_num
pgp_num: 128
docker exec ceph-mon ceph osd pool get testpool-data pg_num
pg_num: 128
docker exec ceph-mon ceph osd pool get testpool-data pgp_num
pgp_num: 128
#查看每个存储池的pg_num
docker exec ceph-mon ceph osd dump |grep pool | awk '{print $1,$3,$4,$5":"$6,$13":"$14}'
pool '.rgw.root' replicated size:3 pg_num:8
pool 'default.rgw.control' replicated size:3 pg_num:8
pool 'default.rgw.meta' replicated size:3 pg_num:8
pool 'default.rgw.log' replicated size:3 pg_num:8
pool 'testpool' replicated size:3 pg_num:128
pool 'testpool-data' replicated size:3 pg_num:128
#ceph osd df 可以查看集群中每个osd的使⽤用信息，以及各个osd上的pg数（288等于以上6个存储池pg_num的总和）
docker exec ceph-mon ceph osd df tree | egrep 'PGS|osd\.'
ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS TYPE NAME
1 hdd 0.14999 1.00000 1800G 171G 1628G 9.54 1.66 288 osd.1
2 hdd 0.14999 1.00000 5553G 351G 5201G 6.34 1.10 288 osd.2
0 hdd 0.14999 1.00000 1795G 1654M 1793G 0.09 0.02 288 osd.0
#⽤用awk计算倒数第⼆二个字段之和，系统总pg数（864 = 288 *3）
docker exec ceph-mon ceph osd df tree | egrep 'PGS|osd\.' | awk '{a+=$(NF-1)} END{print a}'
864
#检查 testpool的replicated size
docker exec ceph-mon ceph osd dump |grep size |grep testpool
pool 5 'testpool' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 100 pgp_num 100 last_change 82 flags hashpspool
stripe_width 0 application cephfs
pool 6 'testpool-data' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 100 pgp_num 100 last_change 82 flags hashpspool
stripe_width 0 application cephfs
#获取存储池的配额
docker exec ceph-mon ceph osd pool get-quota testpool
quotas for pool 'testpool':
max objects: N/A
max bytes : N/A
三、挂载ceph存储池
#在mgr节点上执⾏行行命令，在存储池上建⽴立⽂文件系统：
docker exec ceph-mon ceph fs new cephfs testpool testpool-data
new fs with metadata pool 5 and data pool 6
docker exec ceph-mon ceph fs ls
name: cephfs, metadata pool: testpool, data pools: [testpool-data ]
docker exec ceph-mon ceph-authtool -p /etc/ceph/ceph.client.admin.keyring
AQCtPVlddkJaBBAA5hg10Kcg6BgpFk02k0ACCg==
#在k8s master节点上挂载这个存储池：
yum -y install ceph-fuse ceph
/etc/ceph/admin.key
AQCtPVlddkJaBBAA5hg10Kcg6BgpFk02k0ACCg==
chmod 600 /etc/ceph/admin.key
mount -t ceph 192.168.4.1:6789,192.168.4.9:6789,192.168.4.10,6789:/ /mnt/testpool/ -o name=admin,secretfile=/etc/ceph/admin.key
df命令的结果：
192.168.4.1:6789,192.168.4.9:6789,192.168.4.10,0.0.26.133:/ 9593274368 550637568 9042636800 6% /mnt/testpool
/mnt/testpool⽬目录可读可写，可以建⽴立⽬目录和⽂文件。
#下载dashboard（未成功）
docker run -p 25000:25000 -e CEPHMONS='192.168.4.1,192.168.4.9,192.168.4.10' -e KEYRING="$(sudo cat /etc/ceph/keyring)" crapworks/ceph-dash:latest
四、故障解决记录
1.解决health: HEALTH_WARN 故障
ceph -s命令的输出结果⾥里里，健康状态是HEALTH_WARN：
docker exec ceph-mon ceph -s
cluster:
id: e821ab20-e4d7-4511-acc0-cc76a3ef80f4
health: HEALTH_WARN
561/561 objects misplaced (100.000%)
too few PGs per OSD (10 < min 30)
services:
mon: 3 daemons, quorum br-sh-sip-1,br-sh-docker-1,br-sh-pm-1
mgr: br-sh-sip-1(active), standbys: br-sh-docker-1, br-sh-pm-1
osd: 3 osds: 3 up, 3 in; 32 remapped pgs
rgw: 1 daemon active
data:
pools: 4 pools, 32 pgs
objects: 187 objects, 1113 bytes
usage: 522 GB used, 8625 GB / 9148 GB avail
pgs: 561/561 objects misplaced (100.000%)
32 active+clean+remapped
docker exec ceph-mon ceph -s
cluster:
id: e821ab20-e4d7-4511-acc0-cc76a3ef80f4
health: HEALTH_WARN
657/657 objects misplaced (100.000%)
Reduced data availability: 100 pgs inactive
services:
mon: 3 daemons, quorum br-sh-sip-1,br-sh-docker-1,br-sh-pm-1
mgr: br-sh-sip-1(active), standbys: br-sh-docker-1, br-sh-pm-1
osd: 3 osds: 3 up, 3 in; 32 remapped pgs
rgw: 1 daemon active
data:
pools: 5 pools, 132 pgs
objects: 219 objects, 1113 bytes
usage: 523 GB used, 8625 GB / 9148 GB avail
pgs: 75.758% pgs unknown
657/657 objects misplaced (100.000%)
100 unknown
32 active+clean+remapped
原因：
所有osd节点的pg_num的总和（本例例是864个）要⼩小于mon_max_pg_per_osd变量量的值（默认值是200）。
解决⽅方法：
/home/ceph/etc/ceph/ceph.conf⾥里里增加⼀一⾏行行：
mon_max_pg_per_osd = 1024
重启mon和mgr节点：
scp -r /home/ceph/etc/ceph/ br-sh-docker-1:/home/ceph/etc/
scp -r /home/ceph/etc/ceph/ br-sh-pm-1:/home/ceph/etc/
docker stop ceph-mon
docker rm ceph-mon
docker stop ceph-mgr
docker rm ceph-mgr
docker run -d --net=host --name ceph-mon -v /home/ceph/etc/ceph/:/etc/ceph -v /home/ceph/var/lib/ceph/:/var/lib/ceph -e CEPH_DAEMON=MON -e
MON_IP=192.168.4.1 -e CEPH_PUBLIC_NETWORK=192.168.4.0/24 ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mon
docker run -d --net=host --name ceph-mgr -v /home/ceph/etc/ceph/:/etc/ceph/ -v /home/ceph/var/lib/ceph/:/var/lib/ceph/ -e CEPH_DAEMON=MGR
ceph/daemon:v3.0.5-stable-3.0-luminous-centos-7-x86_64 ceph-mgr
docker ps |grep ceph-mon
docker ps |grep ceph-mgr
重启后查看健康状态是OK。
五、参考
官⽅方⽂文档： https://ceph.io/planet/%e5%9f%ba%e4%ba%8edocker%e9%83%a8%e7%bd%b2ceph%e4%bb%a5%e5%8f%8a%e4%bf
https://ceph.io/planet/基于docker部署ceph以及修改docker-image/
https://blog.csdn.net/u010953692/article/details/77643012 ceph池相关操作
https://www.jianshu.com/p/ae96ee24ef6c 调整ceph的pg数（pg_num， pgp_num）
https://blog.csdn.net/luanpeng825485697/article/details/84678771 helm部署ceph在k8s上做分布式存储
https://opengers.github.io/ceph/ceph-pgs-total-number/ 关于ceph集群中pg总数的计算⽅方法
%ae%e6%94%b9docker-image/ 基于docker部署ceph以及修改docker image
https://drunkard.github.io/cephfs/multimds/ 多活 MDS 守护进程的配置
https://blog.csdn.net/wuxianweizai/article/details/78925479 ceph常⽤用运维技巧总结1
Valid values for CEPH_DAEMON are POPULATE_KVSTORE MON OSD OSD_DIRECTORY OSD_DIRECTORY_SINGLE OSD_CEPH_DISK OSD_CEPH_DISK_PREPARE OSD_CEPH_DISK_ACTIVATE
OSD_CEPH_ACTIVATE_JOURNAL MDS RGW RGW_USER RESTAPI NFS ZAP_DEVICE MON_HEALTH MGR DISK_INTROSPECTION DEMO DISK_LIST TCMU_RUNNER RBD_TARGET_API RBD_TARGET_GW.
Valid values for the daemon parameter are populate_kvstore mon osd osd_directory osd_directory_single osd_ceph_disk osd_ceph_disk_prepare osd_ceph_disk_activate
osd_ceph_activate_journal mds rgw rgw_user restapi nfs zap_device mon_health mgr disk_introspection demo disk_list tcmu_runner rbd_target_api rbd_target_gw
